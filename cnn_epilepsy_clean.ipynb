{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook based off of https://arxiv.org/pdf/1801.05412.pdf , refer to this for parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of necesary librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, Input, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import time\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "from keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilitaries functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_df(letter): #import the .txt files\n",
    "    full_path =\"data/bonn_uni_datasets/\"+ letter + \"/*.*\"\n",
    "    files = files = glob.glob(full_path)\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        df_list.append(pd.read_csv(file, header = None))\n",
    "    big_df = pd.concat(df_list, ignore_index=True, axis= 1)\n",
    "    return big_df.T\n",
    "\n",
    "def norm(X): # zero mean and unit variance normalization\n",
    "    X = X - np.mean(X)\n",
    "    X = X / np.std(X)\n",
    "    return X\n",
    "\n",
    "def window(a, w = 512, o = 64, copy = False): #window sliding function\n",
    "    #default for training, for testing data we will split each signal in four of 1024 and apply\n",
    "    #a window size of 512 with a stride (o) of 256\n",
    "    sh = (a.size - w + 1, w)\n",
    "    st = a.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::o]\n",
    "    if copy:\n",
    "        return view.copy()\n",
    "    else:\n",
    "        return view\n",
    "\n",
    "def enrich_train(df): #enrich data by splicing the 4097-long signals \n",
    "    #into 512 long ones with a stride of 64\n",
    "    labels = df.iloc[:,-1]\n",
    "    data = df.iloc[:, :-1]\n",
    "    res = list()\n",
    "    for i in range(len(data)):\n",
    "        res += [window(data.iloc[i].values)]\n",
    "    return res\n",
    "\n",
    "def reshape_x(arr): #shape the input data into the correct form (x1,x2,1)\n",
    "    nrows = arr.shape[0]\n",
    "    ncols = arr.shape[1]\n",
    "    return arr.reshape(nrows, ncols, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_df():\n",
    "    A = norm(folder_to_df('A'))\n",
    "    B = norm(folder_to_df('B'))\n",
    "    C = norm(folder_to_df('C'))\n",
    "    D = norm(folder_to_df('D'))\n",
    "    E = norm(folder_to_df('E'))\n",
    "    \n",
    "    normal = A.append(B).reset_index(drop = True)\n",
    "    interictal = C.append(D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into 90%/10%, keeping the 10% for the testing of the majority voting later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_data_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train, normal_vote = train_test_split(normal, test_size = 0.1)\n",
    "interictal_train, interictal_vote = train_test_split(interictal, test_size = 0.1)\n",
    "ictal_train, ictal_vote = train_test_split(ictal, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enriching the data as per Scheme 1 in the paper\n",
    "\n",
    "### window sliding with a stride of 64 and length of 512, as well as adding labels and format into the correct shape for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_enrich_train(normal, interictal, ictal):\n",
    "    \n",
    "    #enrich data and reshape it to have a two dimensional array instead of three\n",
    "    normal_train_enr = np.asarray(enrich_train(normal)).reshape(-1, np.asarray(enrich_train(normal)).shape[-1])\n",
    "    interictal_train_enr = np.asarray(enrich_train(interictal)).reshape(-1, np.asarray(enrich_train(interictal)).shape[-1])\n",
    "    ictal_train_enr = np.asarray(enrich_train(ictal)).reshape(-1, np.asarray(enrich_train(ictal)).shape[-1])\n",
    "\n",
    "    #change into a dataframe to add labels easily\n",
    "    normal_train_enr_df = pd.DataFrame(normal_train_enr)\n",
    "    interictal_train_enr_df = pd.DataFrame(interictal_train_enr)\n",
    "    ictal_train_enr_df = pd.DataFrame(ictal_train_enr)\n",
    "    \n",
    "    normal_train_enr_df['labels'] = 0 # normal\n",
    "    interictal_train_enr_df['labels'] = 1 #interictal\n",
    "    ictal_train_enr_df['labels'] = 2 #ictal\n",
    "\n",
    "    #concat all\n",
    "    data_labels = pd.concat([normal_train_enr_df, interictal_train_enr_df, ictal_train_enr_df], ignore_index = True)\n",
    "    \n",
    "\n",
    "    #separates data and labels into numpy arrays for keras\n",
    "    data = data_labels.drop('labels', axis = 1).values\n",
    "    labels = data_labels.labels.values\n",
    "    \n",
    "    #labels = np.expand_dims(labels, axis=1)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model, as per :\n",
    "![Schema of the model](images/model_schema.png)\n",
    "\n",
    "\n",
    "### Parameters taken in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #Conv - 1\n",
    "    model.add(Conv1D(24, 5,strides =  3, input_shape=(512,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #Conv - 2\n",
    "    model.add(Conv1D(16, 3,strides =  2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #Conv - 3\n",
    "    model.add(Conv1D(8, 3,strides =  2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #FC -1\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('relu'))\n",
    "    #Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    #FC -2\n",
    "    model.add(Dense(3,activation = 'softmax'))\n",
    "    #softmax\n",
    "    #model.add(Activation('softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function as well as the stratified 10 fold cross validation for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, xtrain, ytrain, xval, yval, fold):\n",
    "    model_name = 'P-1D-CNN'\n",
    "    checkpointer = ModelCheckpoint(filepath='checkpoints/'+'fold'+ str(fold)+'.'+model_name + '.{epoch:03d}-{acc:.3f}.h5',verbose=0,monitor ='acc', save_best_only=True)\n",
    "    history = model.fit(xtrain, ytrain, batch_size=32, callbacks = [checkpointer],epochs=200, verbose = 1)\n",
    "    print(history)\n",
    "    score = model.evaluate(xval, yval, batch_size=32)\n",
    "    print('\\n')\n",
    "    print(score)\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 10\n",
    "X, y = format_enrich_train(normal, interictal, ictal)\n",
    "#initialize 10 fold validation\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "\n",
    "#10 fold cross validation loop\n",
    "for i, (train, test) in enumerate(skf.split(X,y)):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    start_time = time.time()\n",
    "    X = reshape_x(X)\n",
    "    xtrain, xval = X[train], X[test]\n",
    "    ytrain, yval = y[train], y[test]\n",
    "    ytrain = to_categorical(ytrain, num_classes=3, dtype='float32')\n",
    "    yval = to_categorical(yval, num_classes=3, dtype='float32')\n",
    "\n",
    "\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model()\n",
    "    score, history = train_evaluate_model(model, xtrain, ytrain, xval, yval, i+1)\n",
    "    print(\"Ran \", i+1, \"/\", n_folds, \"Fold in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = load_model('best_model.0.966.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional necessary funtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_vote(df):\n",
    "    res = list()\n",
    "    for i in range(len(df)):\n",
    "        res += [window(df.iloc[i].values,w= 512, o = 256)]\n",
    "    return np.asarray(res)\n",
    "\n",
    "def count_votes(my_list): \n",
    "    freq = {} \n",
    "    for i in my_list: \n",
    "        if (i in freq): \n",
    "            freq[i] += 1\n",
    "        else: \n",
    "            freq[i] = 1\n",
    "    return freq\n",
    "\n",
    "def reshape_signal(signal):\n",
    "    signal = np.expand_dims(signal, axis=1)\n",
    "    signal = np.expand_dims(signal, axis=0)\n",
    "    return np.asarray(signal)\n",
    "\n",
    "def evaluate_subsignals(subsignals,model):\n",
    "    vote_list = np.array([])\n",
    "    for i in range(len(subsignals)):\n",
    "        mini_signal = reshape_signal(subsignals[i])\n",
    "        ynew = model.predict_classes(mini_signal)\n",
    "        vote_list = np.append(vote_list, ynew)\n",
    "    decision = count_votes(vote_list)\n",
    "    return decision_to_str(decision), vote_list\n",
    "\n",
    "def decision_to_str(dec):\n",
    "    res = list()\n",
    "    for key,val in dec.items():\n",
    "        if key == 0:\n",
    "            res += ['normal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 1:\n",
    "            res += ['ictal: ' + str(val) + ' votes' + '\\n']\n",
    "        if key == 2:\n",
    "            res += ['interictal: ' + str(val) + ' votes' + '\\n']\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exctracting 1st normal signal for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_signal = split_vote(ictal_vote)\n",
    "subsignals = big_signal[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is divided into 15 subsignals of length 512, the model will \"vote\" on each subsignal and decide by majority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision, vote_list = evaluate_subsignals(subsignals,best_model)\n",
    "print(vote_list)\n",
    "print(decision[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
