{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential, Model, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding, Activation, Flatten\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_to_df(letter):\n",
    "    full_path =\"data/bonn_uni_datasets/\"+ letter + \"/*.*\"\n",
    "    files = files = glob.glob(full_path)\n",
    "    df_list = []\n",
    "    for file in files:\n",
    "        df_list.append(pd.read_csv(file, header = None))\n",
    "    big_df = pd.concat(df_list, ignore_index=True, axis= 1)\n",
    "    return big_df.T\n",
    "\n",
    "def norm(X):\n",
    "    X = X - np.mean(X)\n",
    "    X = X / np.std(X)\n",
    "    return X\n",
    "\n",
    "def window(a, w = 512, o = 64, copy = False):\n",
    "    #default for training, for testing data we split each signal in four of 1024 and apply\n",
    "    #a window size of 512 with a stride (o) of 256\n",
    "    sh = (a.size - w + 1, w)\n",
    "    st = a.strides * 2\n",
    "    view = np.lib.stride_tricks.as_strided(a, strides = st, shape = sh)[0::o]\n",
    "    if copy:\n",
    "        return view.copy()\n",
    "    else:\n",
    "        return view\n",
    "\n",
    "def enrich_train(df):\n",
    "    labels = df.iloc[:,-1]\n",
    "    data = df.iloc[:, :-1]\n",
    "    res = list()\n",
    "    for i in range(len(data)):\n",
    "        res += [window(data.iloc[i].values)]\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_as_df():\n",
    "    A = norm(folder_to_df('A'))\n",
    "    B = norm(folder_to_df('B'))\n",
    "    C = norm(folder_to_df('C'))\n",
    "    D = norm(folder_to_df('D'))\n",
    "    E = norm(folder_to_df('E'))\n",
    "    \n",
    "    normal = A.append(B).reset_index(drop = True)\n",
    "    interictal = C.append(D).reset_index(drop = True)\n",
    "    ictal = E\n",
    "\n",
    "    return normal, interictal, ictal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal, interictal, ictal = load_data_as_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_train, normal_vote = train_test_split(normal, test_size = 0.1)\n",
    "interictal_train, interictal_vote = train_test_split(interictal, test_size = 0.1)\n",
    "ictal_train, ictal_vote = train_test_split(ictal, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_enrich_train(normal, interictal, ictal):\n",
    "    \n",
    "    normal_train_enr = np.asarray(enrich_train(normal)).reshape(-1, np.asarray(enrich_train(normal)).shape[-1])\n",
    "    interictal_train_enr = np.asarray(enrich_train(interictal)).reshape(-1, np.asarray(enrich_train(interictal)).shape[-1])\n",
    "    ictal_train_enr = np.asarray(enrich_train(ictal)).reshape(-1, np.asarray(enrich_train(ictal)).shape[-1])\n",
    "\n",
    "    normal_train_enr_df = pd.DataFrame(normal_train_enr)\n",
    "    interictal_train_enr_df = pd.DataFrame(interictal_train_enr)\n",
    "    ictal_train_enr_df = pd.DataFrame(ictal_train_enr)\n",
    "    \n",
    "    normal_train_enr_df['labels'] = 0 # normal\n",
    "    interictal_train_enr_df['labels'] = 1 #interictal\n",
    "    ictal_train_enr_df['labels'] = 2 #ictal\n",
    "\n",
    "    data_labels = pd.concat([normal_train_enr_df, interictal_train_enr_df, ictal_train_enr_df], ignore_index = True)\n",
    "    \n",
    "\n",
    "    \n",
    "    data = data_labels.drop('labels', axis = 1).values\n",
    "    labels = data_labels.labels.values\n",
    "    \n",
    "    #labels = np.expand_dims(labels, axis=1)\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    #Conv - 1\n",
    "    model.add(Conv1D(24, 5,strides =  3, input_shape=(512,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #Conv - 2\n",
    "    model.add(Conv1D(16, 3,strides =  2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #Conv - 3\n",
    "    model.add(Conv1D(8, 3,strides =  2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    #FC -1\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(20))\n",
    "    model.add(Activation('relu'))\n",
    "    #Dropout\n",
    "    model.add(Dropout(0.5))\n",
    "    #FC -2\n",
    "    model.add(Dense(3,activation = 'softmax'))\n",
    "    #softmax\n",
    "    #model.add(Activation('softmax'))\n",
    "\n",
    "    adam = optimizers.Adam(lr=0.00002, beta_1=0.9, beta_2=0.999, epsilon=0.00000001, decay=0.0, amsgrad=False)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=adam,\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(model, xtrain, ytrain, xval, yval):\n",
    "    history = model.fit(xtrain, ytrain, batch_size=32, epochs=200)\n",
    "    score = model.evaluate(xval, yval, batch_size=32)\n",
    "    return score, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_x(arr):\n",
    "    nrows = arr.shape[0]\n",
    "    ncols = arr.shape[1]\n",
    "    return arr.reshape(nrows, ncols, 1)\n",
    "\n",
    "def reshape_y(arr):\n",
    "    nrows = arr.shape[0]\n",
    "    ncols = 1\n",
    "    return arr.reshape(nrows, ncols, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 10\n",
      "Epoch 1/200\n",
      "25650/25650 [==============================] - 11s 417us/step - loss: 1.2490 - acc: 0.3503\n",
      "Epoch 2/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 1.0827 - acc: 0.4514\n",
      "Epoch 3/200\n",
      "25650/25650 [==============================] - 11s 419us/step - loss: 1.0040 - acc: 0.5423\n",
      "Epoch 4/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.9315 - acc: 0.6054\n",
      "Epoch 5/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.8766 - acc: 0.6390\n",
      "Epoch 6/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.8400 - acc: 0.6532\n",
      "Epoch 7/200\n",
      "25650/25650 [==============================] - 11s 421us/step - loss: 0.8128 - acc: 0.6670\n",
      "Epoch 8/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.7975 - acc: 0.6731\n",
      "Epoch 9/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.7795 - acc: 0.6824\n",
      "Epoch 10/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.7630 - acc: 0.6896\n",
      "Epoch 11/200\n",
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.7506 - acc: 0.6921\n",
      "Epoch 12/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.7346 - acc: 0.6979\n",
      "Epoch 13/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.7228 - acc: 0.7027\n",
      "Epoch 14/200\n",
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.7060 - acc: 0.7104\n",
      "Epoch 15/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.6993 - acc: 0.7196\n",
      "Epoch 16/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.6850 - acc: 0.7229\n",
      "Epoch 17/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.6782 - acc: 0.7267\n",
      "Epoch 18/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.6630 - acc: 0.7293\n",
      "Epoch 19/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.6502 - acc: 0.7363\n",
      "Epoch 20/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.6338 - acc: 0.7416\n",
      "Epoch 21/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.6209 - acc: 0.7464\n",
      "Epoch 22/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.6053 - acc: 0.7547\n",
      "Epoch 23/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.5939 - acc: 0.7603\n",
      "Epoch 24/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.5755 - acc: 0.7701\n",
      "Epoch 25/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.5671 - acc: 0.7706\n",
      "Epoch 26/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.5521 - acc: 0.7763\n",
      "Epoch 27/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.5383 - acc: 0.7829\n",
      "Epoch 28/200\n",
      "25650/25650 [==============================] - 11s 434us/step - loss: 0.5302 - acc: 0.7904\n",
      "Epoch 29/200\n",
      "25650/25650 [==============================] - 11s 429us/step - loss: 0.5150 - acc: 0.7921\n",
      "Epoch 30/200\n",
      "25650/25650 [==============================] - 11s 436us/step - loss: 0.5088 - acc: 0.7952\n",
      "Epoch 31/200\n",
      "25650/25650 [==============================] - 11s 432us/step - loss: 0.4998 - acc: 0.8028\n",
      "Epoch 32/200\n",
      "25650/25650 [==============================] - 11s 432us/step - loss: 0.4917 - acc: 0.8057\n",
      "Epoch 33/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.4784 - acc: 0.8131\n",
      "Epoch 34/200\n",
      "25650/25650 [==============================] - 11s 439us/step - loss: 0.4716 - acc: 0.8170\n",
      "Epoch 35/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.4609 - acc: 0.8190\n",
      "Epoch 36/200\n",
      "25650/25650 [==============================] - 11s 439us/step - loss: 0.4490 - acc: 0.8244\n",
      "Epoch 37/200\n",
      "25650/25650 [==============================] - 11s 436us/step - loss: 0.4415 - acc: 0.8312\n",
      "Epoch 38/200\n",
      "25650/25650 [==============================] - 11s 435us/step - loss: 0.4318 - acc: 0.8334\n",
      "Epoch 39/200\n",
      "25650/25650 [==============================] - 11s 437us/step - loss: 0.4282 - acc: 0.8378\n",
      "Epoch 40/200\n",
      "25650/25650 [==============================] - 11s 441us/step - loss: 0.4124 - acc: 0.8440\n",
      "Epoch 41/200\n",
      "25650/25650 [==============================] - 11s 441us/step - loss: 0.4026 - acc: 0.8514\n",
      "Epoch 42/200\n",
      "25650/25650 [==============================] - 11s 429us/step - loss: 0.3985 - acc: 0.8514\n",
      "Epoch 43/200\n",
      "25650/25650 [==============================] - 12s 461us/step - loss: 0.3877 - acc: 0.8568\n",
      "Epoch 44/200\n",
      "25650/25650 [==============================] - 12s 471us/step - loss: 0.3818 - acc: 0.8596\n",
      "Epoch 45/200\n",
      "25650/25650 [==============================] - 12s 458us/step - loss: 0.3656 - acc: 0.8673\n",
      "Epoch 46/200\n",
      "25650/25650 [==============================] - 12s 454us/step - loss: 0.3563 - acc: 0.8726\n",
      "Epoch 47/200\n",
      "25650/25650 [==============================] - 12s 480us/step - loss: 0.3458 - acc: 0.8773\n",
      "Epoch 48/200\n",
      "25650/25650 [==============================] - 12s 467us/step - loss: 0.3404 - acc: 0.8801\n",
      "Epoch 49/200\n",
      "25650/25650 [==============================] - 11s 433us/step - loss: 0.3399 - acc: 0.8802\n",
      "Epoch 50/200\n",
      "25650/25650 [==============================] - 11s 429us/step - loss: 0.3303 - acc: 0.8844\n",
      "Epoch 51/200\n",
      "25650/25650 [==============================] - 12s 452us/step - loss: 0.3164 - acc: 0.8873\n",
      "Epoch 52/200\n",
      "25650/25650 [==============================] - 12s 455us/step - loss: 0.3145 - acc: 0.8919\n",
      "Epoch 53/200\n",
      "25650/25650 [==============================] - 11s 445us/step - loss: 0.3032 - acc: 0.8938\n",
      "Epoch 54/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.2922 - acc: 0.8991\n",
      "Epoch 55/200\n",
      "25650/25650 [==============================] - 11s 434us/step - loss: 0.2887 - acc: 0.9008\n",
      "Epoch 56/200\n",
      "25650/25650 [==============================] - 11s 413us/step - loss: 0.2840 - acc: 0.9018\n",
      "Epoch 57/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.2696 - acc: 0.9062\n",
      "Epoch 58/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.2661 - acc: 0.9096\n",
      "Epoch 59/200\n",
      "25650/25650 [==============================] - 10s 394us/step - loss: 0.2660 - acc: 0.9106\n",
      "Epoch 60/200\n",
      "25650/25650 [==============================] - 10s 393us/step - loss: 0.2575 - acc: 0.9134\n",
      "Epoch 61/200\n",
      "25650/25650 [==============================] - 10s 390us/step - loss: 0.2522 - acc: 0.9168\n",
      "Epoch 62/200\n",
      "25650/25650 [==============================] - 10s 391us/step - loss: 0.2455 - acc: 0.9177\n",
      "Epoch 63/200\n",
      "25650/25650 [==============================] - 10s 389us/step - loss: 0.2445 - acc: 0.9197\n",
      "Epoch 64/200\n",
      "25650/25650 [==============================] - 11s 417us/step - loss: 0.2374 - acc: 0.9214\n",
      "Epoch 65/200\n",
      "25650/25650 [==============================] - 11s 437us/step - loss: 0.2350 - acc: 0.9200\n",
      "Epoch 66/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.2269 - acc: 0.9247\n",
      "Epoch 67/200\n",
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.2249 - acc: 0.9242\n",
      "Epoch 68/200\n",
      "25650/25650 [==============================] - 10s 400us/step - loss: 0.2252 - acc: 0.9257\n",
      "Epoch 69/200\n",
      "25650/25650 [==============================] - 11s 418us/step - loss: 0.2200 - acc: 0.9254\n",
      "Epoch 70/200\n",
      "25650/25650 [==============================] - 11s 431us/step - loss: 0.2125 - acc: 0.9287\n",
      "Epoch 71/200\n",
      "25650/25650 [==============================] - 11s 418us/step - loss: 0.2118 - acc: 0.9307\n",
      "Epoch 72/200\n",
      "25650/25650 [==============================] - 10s 402us/step - loss: 0.2071 - acc: 0.9303\n",
      "Epoch 73/200\n",
      "25650/25650 [==============================] - 11s 412us/step - loss: 0.2040 - acc: 0.9324\n",
      "Epoch 74/200\n",
      "25650/25650 [==============================] - 11s 435us/step - loss: 0.2038 - acc: 0.9321\n",
      "Epoch 75/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1981 - acc: 0.9356\n",
      "Epoch 76/200\n",
      "25650/25650 [==============================] - 11s 442us/step - loss: 0.2018 - acc: 0.9331\n",
      "Epoch 77/200\n",
      "25650/25650 [==============================] - 12s 449us/step - loss: 0.1922 - acc: 0.9358\n",
      "Epoch 78/200\n",
      "25650/25650 [==============================] - 12s 455us/step - loss: 0.1942 - acc: 0.9366\n",
      "Epoch 79/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.1940 - acc: 0.9368\n",
      "Epoch 80/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.1949 - acc: 0.9373\n",
      "Epoch 81/200\n",
      "25650/25650 [==============================] - 11s 443us/step - loss: 0.1855 - acc: 0.9397\n",
      "Epoch 82/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.1886 - acc: 0.9377\n",
      "Epoch 83/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.1853 - acc: 0.9398\n",
      "Epoch 84/200\n",
      "25650/25650 [==============================] - 11s 447us/step - loss: 0.1794 - acc: 0.9419\n",
      "Epoch 85/200\n",
      "25650/25650 [==============================] - 11s 440us/step - loss: 0.1831 - acc: 0.9407\n",
      "Epoch 86/200\n",
      "25650/25650 [==============================] - 11s 432us/step - loss: 0.1782 - acc: 0.9428\n",
      "Epoch 87/200\n",
      "25650/25650 [==============================] - 11s 438us/step - loss: 0.1775 - acc: 0.9428\n",
      "Epoch 88/200\n",
      "25650/25650 [==============================] - 11s 420us/step - loss: 0.1789 - acc: 0.9409\n",
      "Epoch 89/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1698 - acc: 0.9456\n",
      "Epoch 90/200\n",
      "25650/25650 [==============================] - 9s 368us/step - loss: 0.1706 - acc: 0.9447\n",
      "Epoch 91/200\n",
      "25650/25650 [==============================] - 9s 362us/step - loss: 0.1728 - acc: 0.9442\n",
      "Epoch 92/200\n",
      "25650/25650 [==============================] - 11s 416us/step - loss: 0.1730 - acc: 0.9423\n",
      "Epoch 93/200\n",
      "25650/25650 [==============================] - 11s 421us/step - loss: 0.1714 - acc: 0.9454\n",
      "Epoch 94/200\n",
      "25650/25650 [==============================] - 11s 417us/step - loss: 0.1685 - acc: 0.9455\n",
      "Epoch 95/200\n",
      "25650/25650 [==============================] - 11s 433us/step - loss: 0.1695 - acc: 0.9454\n",
      "Epoch 96/200\n",
      "25650/25650 [==============================] - 11s 443us/step - loss: 0.1739 - acc: 0.9443\n",
      "Epoch 97/200\n",
      "25650/25650 [==============================] - 11s 426us/step - loss: 0.1630 - acc: 0.9471\n",
      "Epoch 98/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1582 - acc: 0.9482\n",
      "Epoch 99/200\n",
      "25650/25650 [==============================] - 12s 462us/step - loss: 0.1565 - acc: 0.9492\n",
      "Epoch 100/200\n",
      "25650/25650 [==============================] - 11s 431us/step - loss: 0.1631 - acc: 0.9456\n",
      "Epoch 101/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.1610 - acc: 0.9476\n",
      "Epoch 102/200\n",
      "25650/25650 [==============================] - 11s 413us/step - loss: 0.1551 - acc: 0.9498\n",
      "Epoch 103/200\n",
      "25650/25650 [==============================] - 10s 384us/step - loss: 0.1577 - acc: 0.9480\n",
      "Epoch 104/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.1583 - acc: 0.9486\n",
      "Epoch 105/200\n",
      "25650/25650 [==============================] - 10s 391us/step - loss: 0.1578 - acc: 0.9498\n",
      "Epoch 106/200\n",
      "25650/25650 [==============================] - 10s 401us/step - loss: 0.1556 - acc: 0.9490\n",
      "Epoch 107/200\n",
      "25650/25650 [==============================] - 11s 415us/step - loss: 0.1559 - acc: 0.9493\n",
      "Epoch 108/200\n",
      "25650/25650 [==============================] - 10s 391us/step - loss: 0.1494 - acc: 0.9524\n",
      "Epoch 109/200\n",
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.1594 - acc: 0.9495\n",
      "Epoch 110/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1523 - acc: 0.9499\n",
      "Epoch 111/200\n",
      "25650/25650 [==============================] - 11s 442us/step - loss: 0.1518 - acc: 0.9507\n",
      "Epoch 112/200\n",
      "25650/25650 [==============================] - 11s 439us/step - loss: 0.1520 - acc: 0.9495\n",
      "Epoch 113/200\n",
      "25650/25650 [==============================] - 11s 438us/step - loss: 0.1491 - acc: 0.9534\n",
      "Epoch 114/200\n",
      "25650/25650 [==============================] - 11s 430us/step - loss: 0.1516 - acc: 0.9520\n",
      "Epoch 115/200\n",
      "25650/25650 [==============================] - 11s 429us/step - loss: 0.1457 - acc: 0.9525\n",
      "Epoch 116/200\n",
      "25650/25650 [==============================] - 11s 429us/step - loss: 0.1502 - acc: 0.9520\n",
      "Epoch 117/200\n",
      "25650/25650 [==============================] - 11s 414us/step - loss: 0.1436 - acc: 0.9554\n",
      "Epoch 118/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.1455 - acc: 0.9527\n",
      "Epoch 119/200\n",
      "25650/25650 [==============================] - 11s 431us/step - loss: 0.1475 - acc: 0.9519\n",
      "Epoch 120/200\n",
      "25650/25650 [==============================] - 11s 442us/step - loss: 0.1440 - acc: 0.9530\n",
      "Epoch 121/200\n",
      "25650/25650 [==============================] - 11s 442us/step - loss: 0.1448 - acc: 0.9533\n",
      "Epoch 122/200\n",
      "25650/25650 [==============================] - 11s 440us/step - loss: 0.1408 - acc: 0.9544\n",
      "Epoch 123/200\n",
      "25650/25650 [==============================] - 12s 450us/step - loss: 0.1390 - acc: 0.9549\n",
      "Epoch 124/200\n",
      "25650/25650 [==============================] - 11s 413us/step - loss: 0.1376 - acc: 0.9555\n",
      "Epoch 125/200\n",
      "25650/25650 [==============================] - 11s 413us/step - loss: 0.1453 - acc: 0.9531\n",
      "Epoch 126/200\n",
      "25650/25650 [==============================] - 10s 390us/step - loss: 0.1376 - acc: 0.9556\n",
      "Epoch 127/200\n",
      "25650/25650 [==============================] - 11s 419us/step - loss: 0.1443 - acc: 0.9535\n",
      "Epoch 128/200\n",
      "25650/25650 [==============================] - 10s 387us/step - loss: 0.1378 - acc: 0.9549\n",
      "Epoch 129/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.1388 - acc: 0.9554\n",
      "Epoch 130/200\n",
      "25650/25650 [==============================] - 11s 432us/step - loss: 0.1368 - acc: 0.9552\n",
      "Epoch 131/200\n",
      "25650/25650 [==============================] - 11s 440us/step - loss: 0.1355 - acc: 0.9583\n",
      "Epoch 132/200\n",
      "25650/25650 [==============================] - 11s 428us/step - loss: 0.1407 - acc: 0.9551\n",
      "Epoch 133/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.1389 - acc: 0.9552\n",
      "Epoch 134/200\n",
      "25650/25650 [==============================] - 11s 427us/step - loss: 0.1409 - acc: 0.9535\n",
      "Epoch 135/200\n",
      "25650/25650 [==============================] - 11s 425us/step - loss: 0.1318 - acc: 0.9571\n",
      "Epoch 136/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1311 - acc: 0.9566\n",
      "Epoch 137/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1315 - acc: 0.9563\n",
      "Epoch 138/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.1303 - acc: 0.9586\n",
      "Epoch 139/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1368 - acc: 0.9536\n",
      "Epoch 140/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.1315 - acc: 0.9570\n",
      "Epoch 141/200\n",
      "25650/25650 [==============================] - 11s 424us/step - loss: 0.1361 - acc: 0.9553\n",
      "Epoch 142/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.1299 - acc: 0.9582\n",
      "Epoch 143/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1313 - acc: 0.9586\n",
      "Epoch 144/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1323 - acc: 0.9574\n",
      "Epoch 145/200\n",
      "25650/25650 [==============================] - 11s 421us/step - loss: 0.1299 - acc: 0.9561\n",
      "Epoch 146/200\n",
      "25650/25650 [==============================] - 11s 421us/step - loss: 0.1288 - acc: 0.9569\n",
      "Epoch 147/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1298 - acc: 0.9584\n",
      "Epoch 148/200\n",
      "25650/25650 [==============================] - 11s 420us/step - loss: 0.1326 - acc: 0.9574\n",
      "Epoch 149/200\n",
      "25650/25650 [==============================] - 11s 421us/step - loss: 0.1275 - acc: 0.9571\n",
      "Epoch 150/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1276 - acc: 0.9581\n",
      "Epoch 151/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.1294 - acc: 0.9577\n",
      "Epoch 152/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1309 - acc: 0.9581\n",
      "Epoch 153/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1283 - acc: 0.9588\n",
      "Epoch 154/200\n",
      "25650/25650 [==============================] - 11s 420us/step - loss: 0.1246 - acc: 0.9598\n",
      "Epoch 155/200\n",
      "25650/25650 [==============================] - 11s 423us/step - loss: 0.1294 - acc: 0.9602\n",
      "Epoch 156/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.1240 - acc: 0.9594\n",
      "Epoch 157/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25650/25650 [==============================] - 11s 409us/step - loss: 0.1223 - acc: 0.9601\n",
      "Epoch 158/200\n",
      "25650/25650 [==============================] - 11s 412us/step - loss: 0.1283 - acc: 0.9599\n",
      "Epoch 159/200\n",
      "25650/25650 [==============================] - 11s 416us/step - loss: 0.1246 - acc: 0.9600\n",
      "Epoch 160/200\n",
      "25650/25650 [==============================] - 11s 414us/step - loss: 0.1262 - acc: 0.9587\n",
      "Epoch 161/200\n",
      "25650/25650 [==============================] - 11s 415us/step - loss: 0.1222 - acc: 0.9606\n",
      "Epoch 162/200\n",
      "25650/25650 [==============================] - 10s 400us/step - loss: 0.1240 - acc: 0.9592\n",
      "Epoch 163/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1241 - acc: 0.9599\n",
      "Epoch 164/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1250 - acc: 0.9589\n",
      "Epoch 165/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1262 - acc: 0.9592\n",
      "Epoch 166/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 0.1223 - acc: 0.9597\n",
      "Epoch 167/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1255 - acc: 0.9587\n",
      "Epoch 168/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1245 - acc: 0.9603\n",
      "Epoch 169/200\n",
      "25650/25650 [==============================] - 10s 402us/step - loss: 0.1241 - acc: 0.9599\n",
      "Epoch 170/200\n",
      "25650/25650 [==============================] - 10s 402us/step - loss: 0.1263 - acc: 0.9590\n",
      "Epoch 171/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1236 - acc: 0.9576\n",
      "Epoch 172/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1210 - acc: 0.9603\n",
      "Epoch 173/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1196 - acc: 0.9595\n",
      "Epoch 174/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1252 - acc: 0.9602\n",
      "Epoch 175/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1229 - acc: 0.9601\n",
      "Epoch 176/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1259 - acc: 0.9593\n",
      "Epoch 177/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1225 - acc: 0.9596\n",
      "Epoch 178/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.1158 - acc: 0.9620\n",
      "Epoch 179/200\n",
      "25650/25650 [==============================] - 10s 408us/step - loss: 0.1209 - acc: 0.9607\n",
      "Epoch 180/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1250 - acc: 0.9583\n",
      "Epoch 181/200\n",
      "25650/25650 [==============================] - 10s 402us/step - loss: 0.1143 - acc: 0.9625\n",
      "Epoch 182/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 0.1190 - acc: 0.9618\n",
      "Epoch 183/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 0.1182 - acc: 0.9612\n",
      "Epoch 184/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1137 - acc: 0.9630\n",
      "Epoch 185/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.1164 - acc: 0.9617\n",
      "Epoch 186/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1148 - acc: 0.9624\n",
      "Epoch 187/200\n",
      "25650/25650 [==============================] - 10s 409us/step - loss: 0.1165 - acc: 0.9610\n",
      "Epoch 188/200\n",
      "25650/25650 [==============================] - 10s 408us/step - loss: 0.1234 - acc: 0.9591\n",
      "Epoch 189/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.1191 - acc: 0.9606\n",
      "Epoch 190/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.1160 - acc: 0.9622\n",
      "Epoch 191/200\n",
      "25650/25650 [==============================] - 10s 404us/step - loss: 0.1154 - acc: 0.9625\n",
      "Epoch 192/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1189 - acc: 0.9606\n",
      "Epoch 193/200\n",
      "25650/25650 [==============================] - 10s 405us/step - loss: 0.1121 - acc: 0.9639\n",
      "Epoch 194/200\n",
      "25650/25650 [==============================] - 10s 408us/step - loss: 0.1173 - acc: 0.9634\n",
      "Epoch 195/200\n",
      "25650/25650 [==============================] - 10s 403us/step - loss: 0.1153 - acc: 0.9631\n",
      "Epoch 196/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 0.1127 - acc: 0.9625\n",
      "Epoch 197/200\n",
      "25650/25650 [==============================] - 10s 406us/step - loss: 0.1153 - acc: 0.9612\n",
      "Epoch 198/200\n",
      "25650/25650 [==============================] - 10s 408us/step - loss: 0.1134 - acc: 0.9637\n",
      "Epoch 199/200\n",
      "25650/25650 [==============================] - 10s 408us/step - loss: 0.1166 - acc: 0.9620\n",
      "Epoch 200/200\n",
      "25650/25650 [==============================] - 11s 414us/step - loss: 0.1183 - acc: 0.9612\n",
      "2850/2850 [==============================] - 1s 197us/step\n",
      "Ran  1 / 10 Fold in 2167.0487473011017 seconds ---\n",
      "[0.17533373959441698, 0.9424561403508772]\n",
      "\n",
      "\n",
      "<keras.callbacks.History object at 0x7f26ba7bc240>\n",
      "Running Fold 2 / 10\n",
      "Epoch 1/200\n",
      "25650/25650 [==============================] - 11s 411us/step - loss: 1.3148 - acc: 0.3749\n",
      "Epoch 2/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 1.0881 - acc: 0.4654\n",
      "Epoch 3/200\n",
      "25650/25650 [==============================] - 11s 422us/step - loss: 0.9975 - acc: 0.5240\n",
      "Epoch 4/200\n",
      "25650/25650 [==============================] - 10s 407us/step - loss: 0.9310 - acc: 0.5742\n",
      "Epoch 5/200\n",
      "25650/25650 [==============================] - 9s 364us/step - loss: 0.8867 - acc: 0.6046\n",
      "Epoch 6/200\n",
      "25650/25650 [==============================] - 9s 359us/step - loss: 0.8474 - acc: 0.6317\n",
      "Epoch 7/200\n",
      "25650/25650 [==============================] - 10s 402us/step - loss: 0.8251 - acc: 0.6412\n",
      "Epoch 8/200\n",
      " 3488/25650 [===>..........................] - ETA: 9s - loss: 0.8119 - acc: 0.6519"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9856516aad12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;31m# Clearing the NN.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Ran \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Fold in %s seconds ---\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-829d6110aa94>\u001b[0m in \u001b[0;36mtrain_evaluate_model\u001b[0;34m(model, xtrain, ytrain, xval, yval)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_evaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 903\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    904\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    905\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/theano/gof/op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n)\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m             \u001b[0;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_folds = 10\n",
    "X, y = format_enrich_train(normal, interictal, ictal)\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "for i, (train, test) in enumerate(skf.split(X,y)):\n",
    "    print(\"Running Fold\", i+1, \"/\", n_folds)\n",
    "    start_time = time.time()\n",
    "    X = reshape_x(X)\n",
    "    xtrain, xval = X[train], X[test]\n",
    "    ytrain, yval = y[train], y[test]\n",
    "    ytrain = to_categorical(ytrain, num_classes=3, dtype='float32')\n",
    "    yval = to_categorical(yval, num_classes=3, dtype='float32')\n",
    "\n",
    "\n",
    "    model = None # Clearing the NN.\n",
    "    model = create_model()\n",
    "    score, history = train_evaluate_model(model, xtrain, ytrain, xval, yval)\n",
    "    print(\"Ran \", i+1, \"/\", n_folds, \"Fold in %s seconds ---\" % (time.time() - start_time))\n",
    "    print(score)\n",
    "    print('\\n')\n",
    "    print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
